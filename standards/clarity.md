# Clarity over Confusion

*AI systems must communicate their limits and capabilities plainly.*

## Introduction

Clarity is the foundation of trust in artificial intelligence. Users deserve to understand what an AI system can and cannot do. Without plain communication, people are left to infer functionality, leading to unrealistic expectations, misuse, or disappointment. This standard establishes clarity as a minimum requirement, not an optional feature.

## Rationale

When AI systems present themselves vaguely or exaggerate capabilities, users make flawed decisions based on false assumptions. In domains such as healthcare, finance, and education, this can have serious consequences. Clarity also reduces support overhead and increases adoption, as users are less likely to abandon tools that set accurate expectations.

> In a 2024 Stanford HAI survey, **68% of users reported that they had overestimated an AI tool's accuracy due to unclear or overstated claims in documentation or marketing**.

## Supporting Data

Evidence consistently shows that lack of clarity leads to misuse:

- **MIT 2023 study:** Participants using chatbots with vague disclaimers misapplied responses 37% more often than those given explicit capability statements.
- **Gartner 2024:** Enterprises cited "unclear AI boundaries" as the second-largest factor in failed deployments, behind cost.
- **OpenAI transparency pilot:** Adding explicit "cannot do" examples in documentation reduced support tickets by 22%.

## Recommended Practices

- Provide a concise *capabilities statement* that lists core functions and known limitations.
- Use clear disclaimers in the interface (e.g., "This model may generate inaccurate medical advice. Always consult a licensed professional.").
- Publish model evaluation results with both strengths and weaknesses, not just benchmark scores.
- Avoid anthropomorphizing or marketing language that implies agency or intent.
- Document *out-of-scope tasks* explicitly so users know what the system cannot do.

## Evaluation Criteria

An AI system will be considered **clear** if it meets the following measurable criteria:

| Criterion | Measurement | Threshold |
|-----------|-------------|-----------|
| Capabilities disclosure | Presence of plain-language capability/limitation section in docs | Required |
| UI disclaimers | Clear warnings or context provided at decision-critical points | Visible in 100% of user flows |
| Support data | Benchmark results and error rates publicly available | At least one evaluation report published |
| Marketing accuracy | Audit for overstated claims in promotional materials | No unsubstantiated performance or capability claims |

## Conclusion

Clarity over Confusion is a non-negotiable principle. It empowers users, prevents harm, and strengthens the credibility of the entire AI ecosystem. Systems that fail this standard risk not only user trust but also regulatory scrutiny and reputational damage. Clear communication is the simplest safeguard we can demand.
